# Sydney Events Platform

A production-ready MERN stack application that automatically scrapes Sydney events, detects changes, and provides a public-facing UI and admin dashboard with Google OAuth.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLIENT (React/Vite)                   â”‚
â”‚  HomePage â†’ EventCard â†’ EmailModal (public)                  â”‚
â”‚  LoginPage (Google OAuth)                                    â”‚
â”‚  DashboardPage â†’ EventDetailPanel (protected admin)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ Axios (credentials: true)
                        â”‚ /api/*  (Vite proxy â†’ Express)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SERVER (Express)                           â”‚
â”‚                                                              â”‚
â”‚  /api/auth    â†’ Passport.js Google OAuth                     â”‚
â”‚  /api/events  â†’ Public event listing & detail               â”‚
â”‚  /api/admin   â†’ Protected: CRUD, import, scrape trigger      â”‚
â”‚  /api/email   â†’ Email capture with consent                  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚         Scraper Service              â”‚                   â”‚
â”‚  â”‚   eventbriteScraper (Axios/Cheerio)  â”‚â—„â”€â”€â”€ node-cron    â”‚
â”‚  â”‚   whatsOnSydneyScraper (Axios/Cheerio)â”‚    (hourly)      â”‚
â”‚  â”‚   ticketekScraper (Puppeteer)        â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ Mongoose ODM
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MongoDB                                  â”‚
â”‚  Collections: events, users, emailcaptures                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Folder Structure

```
sydney-events/
â”œâ”€â”€ package.json              # Root â€” concurrently dev script
â”œâ”€â”€ docker-compose.yml        # Production containerization
â”‚
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ index.js              # Express app + cron scheduler
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ .env.example
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ passport.js       # Google OAuth strategy
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ Event.js          # Full event schema + indexes
â”‚   â”‚   â”œâ”€â”€ User.js           # Admin user schema
â”‚   â”‚   â””â”€â”€ EmailCapture.js   # Email + consent storage
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ auth.js           # /api/auth/* (Google OAuth)
â”‚   â”‚   â”œâ”€â”€ events.js         # /api/events/* (public)
â”‚   â”‚   â”œâ”€â”€ admin.js          # /api/admin/* (protected)
â”‚   â”‚   â””â”€â”€ email.js          # /api/email/* (capture)
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â””â”€â”€ auth.js           # ensureAuthenticated, ensureAdmin
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ eventbriteScraper.js   # Axios + Cheerio
â”‚   â”‚   â”œâ”€â”€ whatsOnSydneyScraper.js # Axios + Cheerio (2 sources)
â”‚   â”‚   â””â”€â”€ ticketekScraper.js     # Puppeteer (JS-heavy)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ scraperService.js # Orchestration + update detection
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ scrapeOnce.js     # CLI scrape trigger
â”‚
â””â”€â”€ client/
    â”œâ”€â”€ index.html
    â”œâ”€â”€ vite.config.js
    â”œâ”€â”€ tailwind.config.js
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ nginx.conf
    â””â”€â”€ src/
        â”œâ”€â”€ App.jsx            # Router + ProtectedRoute
        â”œâ”€â”€ main.jsx
        â”œâ”€â”€ index.css          # Tailwind + custom tokens
        â”œâ”€â”€ hooks/
        â”‚   â””â”€â”€ useAuth.jsx    # AuthContext provider
        â”œâ”€â”€ services/
        â”‚   â””â”€â”€ api.js         # Axios instance + all API calls
        â”œâ”€â”€ components/
        â”‚   â”œâ”€â”€ Navbar.jsx
        â”‚   â”œâ”€â”€ StatusBadge.jsx
        â”‚   â”œâ”€â”€ EventCard.jsx
        â”‚   â”œâ”€â”€ EmailModal.jsx
        â”‚   â””â”€â”€ EventDetailPanel.jsx
        â””â”€â”€ pages/
            â”œâ”€â”€ HomePage.jsx   # Public event listing
            â”œâ”€â”€ LoginPage.jsx  # Google OAuth redirect
            â”œâ”€â”€ DashboardPage.jsx # Admin (protected)
            â””â”€â”€ NotFoundPage.jsx
```

---

## Scraper Logic

### Sources
1. **Eventbrite** (`eventbriteScraper.js`) â€” Axios + Cheerio against Eventbrite AU listing pages for Music, Food & Drink, and General categories.
2. **What's On Sydney / Sydney.com** (`whatsOnSydneyScraper.js`) â€” Axios + Cheerio against `whatson.cityofsydney.nsw.gov.au` and `sydney.com/events`.
3. **Ticketek** (`ticketekScraper.js`) â€” Puppeteer with headless Chromium for JS-rendered content on `premier.ticketek.com.au`.

### Per-Run Process (in `scraperService.js`)

```
For each scraper:
  1. Fetch raw HTML / render with Puppeteer
  2. Parse event cards â†’ normalize to common shape:
     { title, dateTime, venueName, address, description, imageUrl, sourceUrl, sourceName }
  3. Deduplicate within the scraper run by sourceUrl

For each scraped event:
  â”Œâ”€ sourceUrl NOT in DB? â”€â”€â”€â”€â”€â–º Create event, status = "new"
  â”‚
  â””â”€ sourceUrl EXISTS in DB?
       â”‚
       â”œâ”€ status == "imported"? â”€â”€â–º Only update lastScrapedAt (protect imported status)
       â”‚
       â””â”€ Compare key fields (title, dateTime, venueName, address, description, imageUrl)
              â”‚
              â”œâ”€ Changed? â”€â”€â–º Update fields + status = "updated"
              â””â”€ Unchanged? â”€â”€â–º Only update lastScrapedAt (status preserved)

After processing all scraped events for a source:
  â†’ Any DB event from that sourceName whose sourceUrl was NOT in the current
    scrape run AND status != "imported" â†’ mark status = "inactive"
```

### Change Detection

The `hasEventChanged()` function compares these fields:
- `title` â€” string equality
- `dateTime` â€” ISO string comparison (ignores millisecond noise)
- `venueName` â€” trimmed string equality
- `address` â€” trimmed string equality
- `description` â€” trimmed string equality
- `imageUrl` â€” string equality

---

## Status Lifecycle

```
Scraper finds NEW event         â†’  status: "new"
Scraper finds CHANGED data      â†’  status: "updated"
Admin clicks "Import"           â†’  status: "imported"  (importedAt, importedBy set)
Event NOT found in scrape run   â†’  status: "inactive"
```

**Important rules:**
- `imported` events are **never** automatically changed by the scraper.
- Admins can manually override status via the dropdown in the detail panel.
- `inactive` events are preserved in the DB â€” they don't get deleted.

---

## How to Run Locally

### Prerequisites
- Node.js 20+
- MongoDB (local or MongoDB Atlas)
- Google OAuth credentials (see below)

### 1. Clone & Install

```bash
git clone <repo-url>
cd sydney-events
npm run install:all
```

### 2. Configure Environment

```bash
# Server environment
cp server/.env.example server/.env
# Edit server/.env with your values
```

Required values in `server/.env`:
```
MONGODB_URI=mongodb://localhost:27017/sydney-events
SESSION_SECRET=<random 64-char string>
GOOGLE_CLIENT_ID=<from Google Cloud Console>
GOOGLE_CLIENT_SECRET=<from Google Cloud Console>
GOOGLE_CALLBACK_URL=http://localhost:3000/api/auth/google/callback
CLIENT_URL=http://localhost:5175
```

### 3. Set Up Google OAuth

1. Go to [Google Cloud Console](https://console.cloud.google.com/apis/credentials)
2. Create a new OAuth 2.0 Client ID (Web Application)
3. Add Authorized Redirect URI: `http://localhost:5000/api/auth/google/callback`
4. Copy Client ID and Secret to your `.env`

### 4. Start Development Servers

```bash
# From root directory â€” starts both server + client with hot reload
npm run dev
```

- Frontend: http://localhost:5175+ (may increment if ports busy)
- Backend API: http://localhost:3000

### 5. Trigger Initial Scrape

The server automatically runs a scrape 3 seconds after startup.  
To run manually:

```bash
cd server && npm run scrape
```

Or via the admin dashboard "Scrape Now" button.

---

## Production Deployment (Docker)

```bash
# Create production .env at root
cat > .env << EOF
SESSION_SECRET=your-production-secret
GOOGLE_CLIENT_ID=your-client-id
GOOGLE_CLIENT_SECRET=your-client-secret
GOOGLE_CALLBACK_URL=https://yourdomain.com/api/auth/google/callback
CLIENT_URL=https://yourdomain.com
EOF

# Build and start
docker-compose up --build -d
```

The stack runs:
- MongoDB on port 27017 (internal)
- Express API on port 5000
- Nginx serving React on port 80

---

## API Reference

| Method | Route | Auth | Description |
|--------|-------|------|-------------|
| GET | `/api/auth/google` | â€” | Initiate Google OAuth |
| GET | `/api/auth/google/callback` | â€” | OAuth callback |
| GET | `/api/auth/me` | â€” | Get current user |
| POST | `/api/auth/logout` | â€” | Log out |
| GET | `/api/events` | â€” | Public event list |
| GET | `/api/events/:id` | â€” | Single event |
| POST | `/api/email/capture` | â€” | Capture email + consent |
| GET | `/api/admin/events` | âœ“ | Admin event list with filters |
| POST | `/api/admin/events/:id/import` | âœ“ | Import event to platform |
| PATCH | `/api/admin/events/:id/status` | âœ“ | Update event status |
| DELETE | `/api/admin/events/:id` | âœ“ Admin | Delete event |
| POST | `/api/admin/scrape` | âœ“ Admin | Trigger manual scrape |
| GET | `/api/admin/stats` | âœ“ | Dashboard statistics |
| GET | `/api/admin/emails` | âœ“ Admin | Email captures list |

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| Frontend | React 18, Vite, TailwindCSS, Axios, React Router v6 |
| Backend | Node.js, Express 4 |
| Database | MongoDB 7, Mongoose |
| Auth | Passport.js, passport-google-oauth20, express-session |
| Scraping | Puppeteer (headless Chrome), Axios, Cheerio |
| Scheduling | node-cron |
| Deployment | Docker, Docker Compose, Nginx |

---

## Update Detection Deep Dive

When the scraper runs hourly via node-cron, here's the exact comparison logic:

```javascript
function hasEventChanged(existing, scraped) {
  const fields = ['dateTime', 'venueName', 'description', 'imageUrl', 'title', 'address'];
  
  for (const field of fields) {
    if (field === 'dateTime') {
      // Compare as ISO strings to handle timezone/format differences
      const a = existing.dateTime ? new Date(existing.dateTime).toISOString() : null;
      const b = scraped.dateTime ? new Date(scraped.dateTime).toISOString() : null;
      if (a !== b) return true;
    } else {
      if (String(existing[field] || '').trim() !== String(scraped[field] || '').trim())
        return true;
    }
  }
  return false;
}
```

This ensures:
- Whitespace changes don't falsely trigger `updated`
- Date format normalization prevents false positives
- `null` vs `undefined` treated consistently
- Imported events are never auto-downgraded

---

## Scraping Notes & Limitations

Web scraping is subject to site changes. If a scraper returns 0 results:
1. Check server logs for specific error messages
2. The site's HTML structure may have changed â€” update selectors in the scraper file
3. Some sites may block scrapers â€” consider adding delays or rotating user agents

The platform is designed so that **zero results from a failed scrape does NOT mark all events inactive** â€” the inactive logic only fires when `allProcessedUrls.size > 0`, preventing false mass-inactivation.

---

## Production Readiness Checklist

Before deploying to production, ensure the following are configured:

### âœ… Security
- [ ] **SESSION_SECRET** - Generate a strong random 64+ character string (not the default)
- [ ] **Google OAuth** - Use real credentials from Google Cloud Console, not placeholders
- [ ] **MongoDB** - Use MongoDB Atlas or secured instance with authentication
- [ ] **CORS** - Set `CLIENT_URL` to your actual production domain
- [ ] **Helmet.js** - Security headers already configured âœ“
- [ ] **Rate limiting** - General & auth rate limits already configured âœ“

### âœ… Environment Variables
- [ ] All `.env` values match your production infrastructure
- [ ] Never commit `.env` to version control
- [ ] Use secrets management (AWS Secrets Manager, Vercel Env, etc.)

### âœ… Database
- [ ] MongoDB indexes created (Mongoose handles this automatically)
- [ ] Database backups configured
- [ ] Connection pooling optimized for production traffic

### âœ… Deployment
- [ ] Docker images built and tested
- [ ] `docker-compose.yml` updated with production database URL
- [ ] SSL/TLS certificates configured (use reverse proxy like Nginx)
- [ ] Environment variables configured in deployment platform

### âœ… Monitoring & Logging
- [ ] Add structured logging (e.g., Winston, Pino)
- [ ] Configure error tracking (e.g., Sentry)
- [ ] Set up uptime monitoring
- [ ] Monitor scraper failures

### âš ï¸ Known Limitations
- **Scraper reliability** - Web scraping depends on site structure; sites may block or change selectors
- **Puppeteer overhead** - Ticketek scraper requires headless Chrome; consider horizontal scaling
- **No caching** - Add Redis caching for high-traffic scenarios
- **No pagination optimization** - Implement cursor-based pagination for large datasets
- **Email validation** - Consider adding email verification before accepting captures

### ğŸš€ Recommended Improvements
1. **Testing** - Add test suites (Jest, Mocha) before production
2. **API Documentation** - Generate with Swagger/OpenAPI
3. **Pagination** - Implement cursor-based pagination for event lists
4. **Caching** - Add Redis for frequently accessed data
5. **Queue Jobs** - Use Bull for async scraper jobs instead of node-cron
6. **CDN** - Serve static images through CDN
7. **Analytics** - Track user behavior and event popularity

---
